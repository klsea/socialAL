{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SocialAL Models\n",
    "# Data simulation and parameter recovery - multiple subjects\n",
    "KLS 8.30.19; update 7.7.22  \n",
    "Project info: https://osf.io/b48n2/\n",
    "\n",
    "Model modified from :\n",
    "Fareri, D. S., Chang, L. J., & Delgado, M. R. (2012). Effects of direct social experience on trust decisions and neural reward circuitry. Frontiers in Neuroscience, 6, 1â€“17. https://doi.org/10.3389/fnins.2012.00148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run common_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run priors_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_response(prob):     \n",
    "    n = random.uniform(0,1)\n",
    "    if n > prob:\n",
    "        response = 0\n",
    "    else:\n",
    "        response = 1\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recip_rates = {0: 0.93, 1:0.6, 2:0.07}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New function to simulate data for one sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_data(tn, params):\n",
    "    # tn: number of trials desired\n",
    "    # params: ground truth of parameters\n",
    "    a_gain = params[0]\n",
    "    a_loss = params[1]\n",
    "    beta = params[2]\n",
    "    prob = params[3:6]\n",
    "    \n",
    "    # initialize variables\n",
    "    Probs = [0.5, 0.5, 0.5]\n",
    "    EVs = [[9,9,9,9],[9,9,9,9],[9,9,9,9]]\n",
    "    \n",
    "    # generate trial sequence\n",
    "    trial_sequence = np.repeat([0,1,2], tn)\n",
    "    random.shuffle(trial_sequence) #print(trial_sequence)\n",
    "    \n",
    "    trial = []\n",
    "    choices = []\n",
    "    responses = []\n",
    "    \n",
    "    for x in range(0,len(trial_sequence)):\n",
    "        t = trial_sequence[x] \n",
    "        \n",
    "        # Trial\n",
    "        trial.append(x+1)\n",
    "        \n",
    "        # Make a choice\n",
    "        choice = action_selection(get_action_selection_probs(beta, EVs[t]))\n",
    "        choices.append(choice) \n",
    "    \n",
    "        # Get a response\n",
    "        recip_rate = recip_rates.get(t) \n",
    "        \n",
    "        response = select_response(recip_rate) \n",
    "        responses.append(response)\n",
    "    \n",
    "        # after choice, update probability\n",
    "        if choice != 1:\n",
    "            Probs[t] = update_prob(response, Probs[t], a_gain, a_loss) \n",
    "        # then update value\n",
    "        EVs[t] = update_value(Probs[t]) \n",
    "\n",
    "    data = {'Trial': trial, 'Stim_Sequence': trial_sequence, 'Choice' : choices, 'Trustee_Response': responses}    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim_data(15,[.2,.3,2,.5, .8, .2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create param space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 150\n",
    "a_gains = beta.rvs(a = 1.1, b = 1.1, size = n)\n",
    "a_losses = beta.rvs(a = 1.1, b = 1.1, size = n)\n",
    "betas = gamma.rvs(a = 1.2, scale = 5, size = n)\n",
    "iprobA = uniform.rvs(size = n)\n",
    "iprobB = uniform.rvs(size = n)\n",
    "iprobC = uniform.rvs(size = n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns= ['Trial','Stim_Sequence', 'Choice', 'Trustee_Response', 'Subject', \n",
    "                              'Alpha_gain', 'Alpha_loss','Beta', 'iProbA', 'iProbB', 'iProbC'])\n",
    "for p in range(len(betas)):\n",
    "    dt = sim_data(15, [a_gains[p], a_losses[p], betas[p], iprobA[p], iprobB[p], iprobC[p]])\n",
    "    dt['Subject'] = [p + 1] * 45\n",
    "    dt['Alpha_gain'] = a_gains[p] \n",
    "    dt['Alpha_loss'] = a_losses[p]\n",
    "    dt['Beta'] = betas[p]\n",
    "    dt['iProbA'] = iprobA[p]\n",
    "    dt['iProbB'] =  iprobB[p]\n",
    "    dt['iProbC'] = iprobC[p]\n",
    "    dt = pd.DataFrame(dt)\n",
    "    data = pd.concat([data, dt])\n",
    "data\n",
    "\n",
    "data.to_csv(path_or_buf = '../../output/simulation/sim_2alpha_with_priors_model_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New function to fit model to multiple subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_mult_subject(data):\n",
    "    pdt = pd.DataFrame(columns = ['Subject', 'a_gain', 'a_loss', 'beta', 'iProbA', 'iProbB', 'iProbC'])\n",
    "    a = pd.unique(data['Subject'])\n",
    "    for sub in range(1,len(a)+1):\n",
    "        print('Subject: ', sub)\n",
    "        df = data[data['Subject']==sub]\n",
    "        dt = df.to_dict()\n",
    "        params = model_fit(dt)\n",
    "        #print('Params: ', params)\n",
    "        line = {'Subject': sub, 'a_gain': params.x[0], 'a_loss': params.x[1], 'beta':params.x[2], \n",
    "                'iProbA':params.x[3], 'iProbB':params.x[4], 'iProbC':params.x[5]}\n",
    "        pdt = pdt.append(line, ignore_index=True)   \n",
    "    return(pdt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject:  1\n",
      "Subject:  2\n",
      "Subject:  3\n",
      "Subject:  4\n",
      "Subject:  5\n",
      "Subject:  6\n",
      "Subject:  7\n",
      "Subject:  8\n",
      "Subject:  9\n",
      "Subject:  10\n",
      "Subject:  11\n",
      "Subject:  12\n",
      "Subject:  13\n",
      "Subject:  14\n",
      "Subject:  15\n",
      "Subject:  16\n",
      "Subject:  17\n",
      "Subject:  18\n",
      "Subject:  19\n",
      "Subject:  20\n",
      "Subject:  21\n",
      "Subject:  22\n",
      "Subject:  23\n",
      "Subject:  24\n",
      "Subject:  25\n",
      "Subject:  26\n",
      "Subject:  27\n",
      "Subject:  28\n",
      "Subject:  29\n",
      "Subject:  30\n",
      "Subject:  31\n",
      "Subject:  32\n",
      "Subject:  33\n",
      "Subject:  34\n",
      "Subject:  35\n",
      "Subject:  36\n",
      "Subject:  37\n",
      "Subject:  38\n",
      "Subject:  39\n",
      "Subject:  40\n",
      "Subject:  41\n",
      "Subject:  42\n",
      "Subject:  43\n",
      "Subject:  44\n",
      "Subject:  45\n",
      "Subject:  46\n",
      "Subject:  47\n",
      "Subject:  48\n",
      "Subject:  49\n",
      "Subject:  50\n",
      "Subject:  51\n",
      "Subject:  52\n",
      "Subject:  53\n",
      "Subject:  54\n",
      "Subject:  55\n",
      "Subject:  56\n",
      "Subject:  57\n",
      "Subject:  58\n",
      "Subject:  59\n",
      "Subject:  60\n",
      "Subject:  61\n",
      "Subject:  62\n",
      "Subject:  63\n",
      "Subject:  64\n",
      "Subject:  65\n",
      "Subject:  66\n",
      "Subject:  67\n",
      "Subject:  68\n",
      "Subject:  69\n",
      "Subject:  70\n",
      "Subject:  71\n",
      "Subject:  72\n",
      "Subject:  73\n",
      "Subject:  74\n",
      "Subject:  75\n",
      "Subject:  76\n",
      "Subject:  77\n",
      "Subject:  78\n",
      "Subject:  79\n",
      "Subject:  80\n",
      "Subject:  81\n",
      "Subject:  82\n",
      "Subject:  83\n",
      "Subject:  84\n",
      "Subject:  85\n",
      "Subject:  86\n",
      "Subject:  87\n",
      "Subject:  88\n",
      "Subject:  89\n",
      "Subject:  90\n",
      "Subject:  91\n",
      "Subject:  92\n",
      "Subject:  93\n",
      "Subject:  94\n",
      "Subject:  95\n",
      "Subject:  96\n",
      "Subject:  97\n",
      "Subject:  98\n",
      "Subject:  99\n",
      "Subject:  100\n",
      "Subject:  101\n",
      "Subject:  102\n",
      "Subject:  103\n",
      "Subject:  104\n",
      "Subject:  105\n",
      "Subject:  106\n",
      "Subject:  107\n",
      "Subject:  108\n",
      "Subject:  109\n",
      "Subject:  110\n",
      "Subject:  111\n",
      "Subject:  112\n",
      "Subject:  113\n",
      "Subject:  114\n",
      "Subject:  115\n",
      "Subject:  116\n",
      "Subject:  117\n",
      "Subject:  118\n",
      "Subject:  119\n",
      "Subject:  120\n",
      "Subject:  121\n",
      "Subject:  122\n",
      "Subject:  123\n",
      "Subject:  124\n",
      "Subject:  125\n",
      "Subject:  126\n",
      "Subject:  127\n",
      "Subject:  128\n",
      "Subject:  129\n",
      "Subject:  130\n",
      "Subject:  131\n",
      "Subject:  132\n",
      "Subject:  133\n",
      "Subject:  134\n",
      "Subject:  135\n",
      "Subject:  136\n",
      "Subject:  137\n",
      "Subject:  138\n",
      "Subject:  139\n",
      "Subject:  140\n",
      "Subject:  141\n",
      "Subject:  142\n",
      "Subject:  143\n",
      "Subject:  144\n",
      "Subject:  145\n",
      "Subject:  146\n",
      "Subject:  147\n",
      "Subject:  148\n",
      "Subject:  149\n",
      "Subject:  150\n"
     ]
    }
   ],
   "source": [
    "precover = model_fit_mult_subject(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Subject    a_gain    a_loss      beta    iProbA    iProbB        iProbC\n",
      "0        1.0  0.399974  0.016708  0.717467  1.000000  0.967659  3.108200e-01\n",
      "1        2.0  0.183169  0.175354  0.786023  0.982873  0.730322  3.633818e-01\n",
      "2        3.0  0.883858  0.575956  0.735766  0.071290  0.423661  5.376246e-02\n",
      "3        4.0  0.101933  0.894415  0.763406  0.981018  0.447260  1.057424e-03\n",
      "4        5.0  0.433945  0.032398  0.406314  0.977758  0.219832  1.721306e-01\n",
      "5        6.0  0.339935  0.584057  0.678062  0.914326  0.102547  2.207902e-01\n",
      "6        7.0  0.304591  0.256960  0.925429  0.858450  0.231195  4.917106e-01\n",
      "7        8.0  0.605948  0.575934  0.901035  0.997779  0.060357  3.203206e-01\n",
      "8        9.0  0.686513  0.139550  0.109666  0.985290  0.954674  4.263039e-01\n",
      "9       10.0  0.927101  0.560670  0.769910  0.438016  0.126796  3.630161e-02\n",
      "10      11.0  0.351146  0.391556  0.799789  0.998494  0.117896  2.155882e-01\n",
      "11      12.0  0.401005  0.274463  0.744512  0.999578  0.775455  6.914020e-01\n",
      "12      13.0  0.047507  0.293804  0.999991  0.555795  0.491472  1.051683e-01\n",
      "13      14.0  0.634016  0.014986  0.407677  0.999997  0.769584  9.795014e-01\n",
      "14      15.0  0.710098  0.246658  2.066202  1.000000  0.483385  3.470579e-01\n",
      "15      16.0  0.274565  0.649807  0.214347  0.903860  0.736301  9.439199e-02\n",
      "16      17.0  0.686783  0.070176  0.867243  1.000000  0.929627  2.254816e-01\n",
      "17      18.0  0.066570  0.145060  0.746928  0.938863  0.249009  2.787835e-01\n",
      "18      19.0  0.405400  0.056032  0.901004  0.999915  0.153014  3.430424e-01\n",
      "19      20.0  0.035033  0.381841  0.155246  0.773120  0.647597  2.934183e-02\n",
      "20      21.0  0.186310  0.326941  0.740800  0.487763  0.298911  1.197711e-01\n",
      "21      22.0  0.863268  0.205659  0.517065  0.793922  0.160357  2.383687e-01\n",
      "22      23.0  0.520167  0.264847  0.102371  0.999205  0.412113  1.569100e-01\n",
      "23      24.0  0.710423  0.547121  0.555306  0.999998  0.373440  1.363852e-01\n",
      "24      25.0  0.282611  0.079369  0.074707  0.563398  0.801554  4.358288e-01\n",
      "25      26.0  0.197818  0.934153  0.371081  0.776998  0.880549  6.951761e-08\n",
      "26      27.0  0.690439  0.524712  0.561765  0.995572  0.130605  2.116880e-01\n",
      "27      28.0  0.012400  0.355169  0.526017  0.371827  0.119978  2.049389e-01\n",
      "28      29.0  0.223320  0.062657  0.392432  0.937359  0.635411  3.650649e-01\n",
      "29      30.0  0.066896  0.227801  0.544996  0.852919  0.860238  3.042117e-01\n",
      "..       ...       ...       ...       ...       ...       ...           ...\n",
      "120    121.0  0.483890  0.551044  0.672925  0.974637  0.696755  6.704160e-02\n",
      "121    122.0  0.874971  0.469666  0.794935  0.529846  1.000000  7.346933e-02\n",
      "122    123.0  0.515025  0.179104  0.254205  0.999999  0.702518  1.949655e-01\n",
      "123    124.0  0.837581  0.544588  0.965038  0.999991  0.448790  1.589313e-01\n",
      "124    125.0  0.986294  0.766420  0.570422  0.063292  0.233547  1.177535e-02\n",
      "125    126.0  0.050264  0.503624  0.027854  0.301363  0.348165  1.868606e-02\n",
      "126    127.0  0.394625  0.530886  1.839602  0.281716  0.360508  2.273527e-01\n",
      "127    128.0  0.864957  0.754786  0.448615  0.999863  0.043274  1.048039e-03\n",
      "128    129.0  0.309138  0.564135  0.383781  0.107663  0.946909  1.747096e-01\n",
      "129    130.0  0.634855  0.405752  0.965845  0.484620  0.582157  1.163071e-01\n",
      "130    131.0  0.072694  0.215176  0.750007  0.737490  0.852649  1.651151e-01\n",
      "131    132.0  0.387201  0.173536  0.510735  0.843730  0.460070  3.207431e-01\n",
      "132    133.0  0.155016  0.350755  0.720534  0.270254  0.195690  7.949255e-01\n",
      "133    134.0  0.742205  0.072639  0.775243  0.313864  0.851511  1.104932e-01\n",
      "134    135.0  0.579099  0.533394  0.769965  0.987809  0.338875  1.249361e-01\n",
      "135    136.0  0.016388  0.406452  0.812587  0.135741  0.234575  2.327017e-01\n",
      "136    137.0  0.130067  0.382750  0.443953  0.991259  0.395757  1.109616e-01\n",
      "137    138.0  0.966503  0.002201  0.772833  0.999973  0.995527  7.854596e-03\n",
      "138    139.0  0.032078  0.626253  0.247858  0.817645  0.991193  2.954636e-09\n",
      "139    140.0  0.024842  0.532122  0.629512  0.575978  0.097885  7.929125e-02\n",
      "140    141.0  0.316015  0.930122  0.326623  0.980964  0.001913  4.178148e-03\n",
      "141    142.0  0.561965  0.438407  0.963963  0.995371  0.508112  1.742551e-01\n",
      "142    143.0  0.342061  0.227396  0.099231  0.691341  0.808716  6.989085e-02\n",
      "143    144.0  0.249155  0.572235  0.549827  0.989886  0.974012  6.045649e-02\n",
      "144    145.0  0.537936  0.246398  0.451258  0.881878  0.432421  2.487337e-02\n",
      "145    146.0  0.955756  0.213455  0.957917  1.000000  0.786205  4.753017e-01\n",
      "146    147.0  0.690843  0.681993  0.603269  0.999665  0.099059  9.099794e-02\n",
      "147    148.0  0.764571  0.654670  0.410174  0.342374  0.079792  8.823950e-02\n",
      "148    149.0  0.681580  0.490792  0.295126  1.000000  0.056835  4.858452e-03\n",
      "149    150.0  0.142916  0.052107  0.592057  0.979583  0.925943  4.355473e-01\n",
      "\n",
      "[150 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(precover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "precover.to_csv(path_or_buf = '../../output/simulation/sim_2alpha_with_priors_model_fit.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
